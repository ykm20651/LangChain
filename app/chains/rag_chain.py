
"""
RAGChain - Retrieval-Augmented Generation (ê²€ìƒ‰ ê¸°ë°˜ ìƒì„± ì²´ì¸)

ë­ì²´ì¸ í‘œì¤€ êµ¬ì¡°ë¡œ Retriever â†’ PromptTemplate â†’ ChatOpenAI â†’ StrOutputParser

1) Retrieverë¥¼ ì´ìš©í•´ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰
2) ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ context ë¬¸ìì—´ë¡œ ë³‘í•©
3) PromptTemplateìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
4) ChatOpenAIë¡œ LLM í˜¸ì¶œ
5) StrOutputParserë¡œ ê²°ê³¼ í…ìŠ¤íŠ¸ ë°˜í™˜

ì‚¬ìš© ì˜ˆì‹œ:
    rag = RAGChain(collection="marine_laws", top_k=5)
    answer = rag.run("ìœ ë¥˜ìœ ì¶œ ì‚¬ê³ ì˜ ë³´í—˜ ì²˜ë¦¬ ì ˆì°¨ë¥¼ ìš”ì•½í•´ì¤˜")

"""

from typing import List, Optional
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableSequence

from app.services.vectorstore import VectorStoreService


class RAGChain:
    def __init__(
        self,
        collection: str = "default",
        top_k: int = 4,
        model: str = "gpt-4o-mini",
        temperature: float = 0.2,
        max_context_len: int = 3000,
        include_sources: bool = True,
        debug: bool = False,
    ):
        """
        RAG ì²´ì¸ ì´ˆê¸°í™”
        Args:
            collection: ë²¡í„°ìŠ¤í† ì–´ ì»¬ë ‰ì…˜ëª…
            top_k: ê²€ìƒ‰ ë¬¸ì„œ ê°œìˆ˜
            model: ì‚¬ìš©í•  OpenAI LLM ëª¨ë¸ëª…
            temperature: ì°½ì˜ì„± (ë‚®ì„ìˆ˜ë¡ ê°ê´€ì )
            max_context_len: í”„ë¡¬í”„íŠ¸ì— ë„£ì„ ìµœëŒ€ ë¬¸ë§¥ ê¸¸ì´
            include_sources: ê²°ê³¼ í•˜ë‹¨ì— ë¬¸ì„œ ì¶œì²˜ ìš”ì•½ í¬í•¨ ì—¬ë¶€
            debug: Trueë©´ ê²€ìƒ‰ëœ ë¬¸ì„œ ë° context ì½˜ì†” ì¶œë ¥
        """
        self.collection = collection
        self.top_k = top_k
        self.include_sources = include_sources
        self.debug = debug
        self.max_context_len = max_context_len

        # (1) ë²¡í„°ìŠ¤í† ì–´ ì„œë¹„ìŠ¤ ë¡œë“œ
        self.vs = VectorStoreService()
        self.retriever = self.vs.get_retriever(collection=collection, k=top_k)

        # (2) LLM (OpenAI)
        self.llm = ChatOpenAI(model=model, temperature=temperature)

        # (3) ì¶œë ¥ íŒŒì„œ
        self.parser = StrOutputParser()

        # (4) í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.prompt = ChatPromptTemplate.from_template(
            """
ë„ˆëŠ” í•´ì–‘ ë³´í—˜ ì²­êµ¬ ë³´ê³ ì„œ ì‘ì„± ë³´ì¡° AIë‹¤.
ì•„ë˜ [ê²€ìƒ‰ ë¬¸ë§¥]ì— í¬í•¨ëœ ì •ë³´ë¥¼ ì‚¬ì‹¤ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ë¼.
ë§Œì•½ ê´€ë ¨ ì •ë³´ê°€ ì—†ë‹¤ë©´ "ê·¼ê±° ë¶€ì¡±"ì´ë¼ê³  ë‹µí•˜ë¼.
ê°€ëŠ¥í•˜ë©´ ë¬¸ì„œ ì¶œì²˜ë¥¼ ìš”ì•½í•´ì„œ í•˜ë‹¨ì— ì •ë¦¬í•˜ë¼.

[ê²€ìƒ‰ ë¬¸ë§¥]
{context}

[ì§ˆë¬¸]
{question}
"""
        )

        # (5) Runnable ì‹œí€€ìŠ¤ êµ¬ì„±
        #    question â†’ retriever â†’ prompt â†’ llm â†’ parser
        self.chain = (
            {"context": self._context_from_retriever, "question": RunnablePassthrough()}
            | self.prompt
            | self.llm
            | self.parser
        )

    # -----------------------------
    # ë‚´ë¶€ ë©”ì„œë“œ
    # -----------------------------

    def _context_from_retriever(self, question: str) -> str:
        """
        Retrieverë¡œ ë¬¸ì„œ ê²€ìƒ‰ â†’ context ë¬¸ìì—´ ìƒì„±
        """
        docs: List[Document] = self.retriever.get_relevant_documents(question)

        if self.debug:
            print("\nğŸ” [RAG ê²€ìƒ‰ ê²°ê³¼] ----------------------")
            for d in docs:
                print(f"â€¢ {d.metadata.get('source', 'unknown')}: {d.page_content[:120]}...")
            print("----------------------------------------\n")

        # ë¬¸ì„œ ë‚´ìš© í•©ì¹˜ê¸°
        merged = "\n\n".join([d.page_content for d in docs])
        if len(merged) > self.max_context_len:
            merged = merged[: self.max_context_len] + "\n\n...(ë¬¸ë§¥ ê¸¸ì´ ì´ˆê³¼ë¡œ ì¼ë¶€ ìƒëµë¨)"

        # ë¬¸ì„œ ì¶œì²˜ ì¶”ê°€
        if self.include_sources:
            sources = "\n".join(
                [f"- {d.metadata.get('source', 'unknown')}" for d in docs]
            )
            merged += f"\n\n[ì°¸ê³  ë¬¸ì„œ ì¶œì²˜]\n{sources}"

        return merged

    # -----------------------------
    # í¼ë¸”ë¦­ ë©”ì„œë“œ
    # -----------------------------

    def run(self, question: str) -> str:
        """
        ë‹¨ì¼ ì§ˆë¬¸ì— ëŒ€í•´ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰.
        ê²€ìƒ‰ + ìƒì„± ê²°ê³¼ ë¬¸ìì—´ ë°˜í™˜.
        """
        return self.chain.invoke(question)

    def run_with_sources(self, question: str) -> dict:
        """
        ê²€ìƒ‰ëœ ë¬¸ì„œ ì¶œì²˜ê¹Œì§€ í•¨ê»˜ ë°˜í™˜í•˜ëŠ” ë²„ì „.
        """
        docs: List[Document] = self.retriever.get_relevant_documents(question)
        context = "\n\n".join([d.page_content for d in docs])
        answer = self.llm.invoke(self.prompt.format(context=context, question=question)).content

        return {
            "answer": answer,
            "sources": [d.metadata.get("source", "unknown") for d in docs],
        }
